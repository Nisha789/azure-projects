# ğŸ’³ Fintech SQL Data Migration Pipeline

## ğŸ“– Project Overview
The **Fintech SQL Data Migration Pipeline** is designed to extract financial datasets (Customers, Accounts, Loans, Payments, and Transactions) from **Azure SQL Database**, stage them in **Azure Data Lake Storage (ADLS)**, and transform them into optimized, analytics-ready **Delta tables** using **Azure Synapse** and **PySpark notebooks**.

This project demonstrates a **modern data lakehouse approach**, following the **Bronze â†’ Silver â†’ Gold** architecture for reliability, scalability, and performance.

<!-- --- -->

## ğŸ› ï¸ Tools and Technologies

- Azure Data Lake Storage (ADLS) â€“ Hierarchical data lake with bronze, silver, and gold containers  
- Azure SQL Database â€“ Source system for financial datasets  
- Azure Synapse Analytics â€“ Orchestration and pipeline management  
- PySpark â€“ Data transformation and enrichment (Bronze â†’ Silver â†’ Gold)  
- Delta Lake â€“ Storage format for reliable, ACID-compliant tables  
- Azure Notifications â€“ Email alerts for pipeline success or failure  
- SQL â€“ Schema creation and downstream analytical queries  

<!-- --- -->

## ğŸš€ Architecture Overview
```plaintext

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Azure SQL Database â”‚
         â”‚ (Source Tables)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
           [Synapse Pipeline]
                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 1. Lookup Activity         â”‚
     â”‚ 2. ForEach Copy to Bronze  â”‚
     â”‚ 3. PySpark Notebooks       â”‚
     â”‚    (Bronze â†’ Silver â†’ Gold)â”‚
     â”‚ 4. Notifications (Email)   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Azure Data Lake Storage     â”‚
      â”‚ bronze/ â†’ silver/ â†’ gold/   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<!-- --- -->

## ğŸ“Š Data Flow Steps

### 1. Ingestion â€“ Azure SQL â†’ Bronze
- Source tables: Customers, Accounts, Loans, Payments, Transactions  
- Synapse pipeline Lookup activity retrieves the table list dynamically  
- ForEach activity copies data from SQL to Bronze ADLS containers in Parquet format  

### 2. Transformation â€“ Bronze â†’ Silver
- Data cleaning and enrichment performed using PySpark notebooks  
- Sensitive fields such as emails are masked  
- Derived fields such as total interest, loan duration, and transaction category are calculated  

### 3. Data Modeling â€“ Silver â†’ Gold
- Transforms data into a dimensional star schema for analytics  
- Dimension tables: dim_customers, dim_accounts, dim_loans  
- Fact tables: fact_payments, fact_transactions  
- Designed for BI and reporting tools to consume efficiently  

### 4. Notification & Monitoring
- Success and failure events trigger email notifications  
- Provides operational visibility for data engineers  

<!-- --- -->

## âš™ï¸ Project Workflow

1. Setup ADLS: Create bronze, silver, and gold containers  
2. Deploy Azure SQL Source: Create tables and populate with mock financial data  
3. Create Synapse Pipeline: Lookup + ForEach activities for ingestion, notebook activities for transformations  
4. Run PySpark Notebooks: Bronze â†’ Silver for cleaning, Silver â†’ Gold for modeling  
5. Verify Output: Confirm curated data in the gold container ready for analytics  

<!-- --- -->

## ğŸ§  Key Optimizations & Design Decisions

- Layered Architecture (Bronze â†’ Silver â†’ Gold) improves governance and quality  
- Delta format ensures ACID compliance, schema evolution, and time travel  
- Modular notebooks allow each dataset to be transformed independently  
- Scalable Synapse pipeline pattern can easily extend to more datasets  

<!-- --- -->

## ğŸ”® Future Enhancements

- Power BI integration for real-time dashboards  
- Data quality framework with automated validation rules  
- Parameterization to make table list and paths fully dynamic  
- CI/CD automation using Azure DevOps pipelines  

## ğŸ“‚ Repository Link

ğŸ”— [GitHub Repository](https://github.com/Nisha789/azure-projects/tree/main/fintech-data-migration-project)


## ğŸ¤ Contributions

Contributions are welcome!  
If you'd like to contribute, please **fork** the repository and submit a **pull request** with your enhancements.


## ğŸ“„ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](./LICENSE) file for details.

## ğŸ“š References

- [Azure Data Lake Storage Documentation](https://learn.microsoft.com/en-us/azure/storage/data-lake-storage/)  
- [Azure Synapse Analytics Documentation](https://learn.microsoft.com/en-us/azure/synapse-analytics/)  
- [PySpark Official Documentation](https://spark.apache.org/docs/latest/api/python/)  
- [Delta Lake Documentation](https://delta.io/)  
- [Azure SQL Database Documentation](https://learn.microsoft.com/en-us/azure/azure-sql/)

## ğŸ™ Acknowledgments

- Special thanks to the **Azure team** for providing services like **Synapse Analytics**, **Data Lake Storage**, and **Azure SQL Database**.  
- Thanks to the **open-source community** for PySpark and Delta Lake, which make large-scale data transformation efficient.  
- Inspired by modern **data lakehouse architecture patterns** for designing scalable ETL pipelines.


## ğŸ™‹â€â™€ï¸ Author

**Nisha S**  
Azure Data Engineer | Designing and optimizing real-time data pipelines and scalable analytics solutions.

ğŸ“« [LinkedIn](https://www.linkedin.com/in/nisha-data-ai/)
